{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒ¡ ClimaHealth AI â€” Climate-Driven Disease Outbreak Early Warning System\n",
        "### InnovAIte Hackathon 2026 | Northeastern University AI Club\n",
        "\n",
        "This notebook runs the full ML pipeline:\n",
        "1. Clone the repo from GitHub\n",
        "2. Fetch real data from NASA POWER, WHO GHO, and GDELT APIs\n",
        "3. Train all 3 ML models\n",
        "4. Run a live demo prediction\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone the Repo & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# OPTION A: Clone from GitHub (update URL to your repo)\n",
        "# ============================================\n",
        "# !git clone https://github.com/YOUR_USERNAME/climahealth-ai.git\n",
        "# %cd climahealth-ai/backend\n",
        "\n",
        "# ============================================\n",
        "# OPTION B: Upload the zip file manually\n",
        "# ============================================\n",
        "# 1. Click the folder icon on the left sidebar\n",
        "# 2. Upload climahealth-ai.zip\n",
        "# 3. Run:\n",
        "# !unzip climahealth-ai.zip\n",
        "# %cd climahealth-ai/backend\n",
        "\n",
        "# ============================================\n",
        "# OPTION C: Upload from Google Drive\n",
        "# ============================================\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp /content/drive/MyDrive/climahealth-ai.zip .\n",
        "# !unzip climahealth-ai.zip\n",
        "# %cd climahealth-ai/backend\n",
        "\n",
        "# >>> UNCOMMENT ONE OF THE OPTIONS ABOVE, THEN RUN THIS CELL <<<"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (most are pre-installed in Colab)\n",
        "!pip install -q requests pandas scikit-learn joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify we're in the right directory\n",
        "import os\n",
        "print('Current directory:', os.getcwd())\n",
        "print('Files:', os.listdir('.'))\n",
        "assert os.path.exists('train.py'), 'âŒ Not in the backend folder. Run: %cd climahealth-ai/backend'\n",
        "print('âœ… Ready to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fetch Real Data from APIs\n",
        "This pulls real climate data from NASA, disease data from WHO, and news from GDELT.\n",
        "\n",
        "**All APIs are free and require no authentication.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python fetch_real_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Models on Real Data\n",
        "Trains all 3 ML models:\n",
        "- Climate Forecaster (Gradient Boosting time-series)\n",
        "- Disease Predictor (Random Forest + Gradient Boosting + Logistic Regression ensemble)\n",
        "- NLP Outbreak Detector (TF-IDF + Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python train_real.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Train on Synthetic Data (if APIs are down)\n",
        "Skip this if Step 2 & 3 worked. This generates synthetic data and trains offline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only run this if the API fetch above failed\n",
        "# !python train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 4: Run a Live Demo Prediction\n",
        "Load the trained models and run a prediction for Dhaka, Bangladesh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from models.climate_forecaster import ClimateForecaster\n",
        "from models.disease_predictor import DiseasePredictor\n",
        "from models.nlp_detector import OutbreakSignalDetector\n",
        "from models.ensemble import EnsembleRiskEngine\n",
        "\n",
        "# Load trained models\n",
        "MODEL_DIR = 'saved_models'\n",
        "climate_model = ClimateForecaster.load(MODEL_DIR)\n",
        "disease_model = DiseasePredictor.load(MODEL_DIR)\n",
        "nlp_model = OutbreakSignalDetector.load(MODEL_DIR)\n",
        "ensemble = EnsembleRiskEngine(climate_model, disease_model, nlp_model)\n",
        "\n",
        "print('âœ… All models loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate demo data for Dhaka\n",
        "from data.generate_training_data import (\n",
        "    REGION_PROFILES, generate_climate_timeseries,\n",
        "    generate_disease_incidence, generate_nlp_signals,\n",
        "    create_feature_matrix, compute_risk_score\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "profile = REGION_PROFILES['dhaka_bangladesh']\n",
        "climate_df = generate_climate_timeseries(profile, n_weeks=520)\n",
        "cases = generate_disease_incidence(climate_df, profile['diseases']['dengue'], n_weeks=520)\n",
        "nlp_signals = generate_nlp_signals(climate_df, cases, n_weeks=520)\n",
        "features_df = create_feature_matrix(climate_df, nlp_signals, lag_weeks=4)\n",
        "\n",
        "# Align targets\n",
        "aligned_cases = cases[len(cases) - len(features_df):]\n",
        "aligned_risk = compute_risk_score(cases)[len(cases) - len(features_df):]\n",
        "features_df['cases'] = aligned_cases\n",
        "features_df['risk_score'] = aligned_risk\n",
        "features_df['outbreak'] = (aligned_risk >= 60).astype(int)\n",
        "features_df['region'] = 'dhaka_bangladesh'\n",
        "features_df['disease'] = 'dengue'\n",
        "\n",
        "print(f'âœ… Demo data ready: {len(features_df)} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ensemble prediction\n",
        "demo_headlines = [\n",
        "    'WHO reports surge in dengue cases across Bangladesh',\n",
        "    'Dhaka hospitals overwhelmed by fever patients amid monsoon season',\n",
        "    'Aedes mosquito breeding sites multiply after heavy rainfall in Dhaka slums',\n",
        "    'New hospital opens in Dhaka with expanded ICU capacity',\n",
        "]\n",
        "\n",
        "assessment = ensemble.assess_risk(\n",
        "    climate_df=climate_df,\n",
        "    features_df=features_df,\n",
        "    news_texts=demo_headlines,\n",
        ")\n",
        "\n",
        "print('\\n' + '='*55)\n",
        "print('  ENSEMBLE RISK ASSESSMENT â€” DHAKA, BANGLADESH')\n",
        "print('='*55)\n",
        "print(f\"  Risk Score:       {assessment['risk_score']}/100\")\n",
        "print(f\"  Risk Level:       {assessment['risk_level'].upper()}\")\n",
        "print(f\"  Confidence:       {assessment['confidence']*100:.1f}%\")\n",
        "print(f\"  Outbreak Prob:    {assessment['outbreak_probability']*100:.1f}%\")\n",
        "print(f'\\n  Component Scores:')\n",
        "for comp, score in assessment['component_scores'].items():\n",
        "    print(f'    {comp:30s} {score:>5.1f}/100')\n",
        "print(f'\\n  SHAP Feature Importance:')\n",
        "for cat, imp in assessment['shap_summary'].items():\n",
        "    bar = 'â–ˆ' * int(imp * 40)\n",
        "    print(f'    {cat:20s} {imp*100:5.1f}% {bar}')\n",
        "print('='*55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8-Week Climate Forecast\n",
        "print('\\nðŸ“Š 8-Week Climate Forecast:')\n",
        "print(f'{\"Week\":<6} {\"Temp (Â°C)\":<12} {\"Precip (mm)\":<12}')\n",
        "print('-' * 30)\n",
        "for f in assessment['climate_forecast']:\n",
        "    print(f\"{f['week']:<6} {f['temperature']:<12} {f['precipitation']:<12}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NLP Signal Detection Results\n",
        "print('\\nðŸ“° NLP Signal Analysis:')\n",
        "for sig in assessment['nlp_signals']:\n",
        "    emoji = 'ðŸ”´ OUTBREAK' if sig['is_outbreak'] else 'ðŸŸ¢ Normal'\n",
        "    print(f\"  {emoji} (conf={sig['confidence']:.2f}, sev={sig['severity']})\")\n",
        "    print(f\"    \\\"{sig['text'][:70]}...\\\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Community Health Worker Alert\n",
        "chw = ensemble.generate_chw_alert(assessment, 'Dhaka, Bangladesh', 'Dengue Fever')\n",
        "print('\\nðŸ“‹ COMMUNITY HEALTH WORKER ALERT')\n",
        "print('=' * 55)\n",
        "print(f\"Region:     {chw['region']}\")\n",
        "print(f\"Disease:    {chw['disease']}\")\n",
        "print(f\"Risk Level: {chw['risk_level'].upper()}\")\n",
        "print(f\"Score:      {chw['risk_score']}/100\")\n",
        "print(f\"\\nSummary: {chw['summary']}\")\n",
        "print(f\"\\nRecommended Actions:\")\n",
        "for i, action in enumerate(chw['recommended_actions'], 1):\n",
        "    print(f'  {i}. {action}')\n",
        "print(f\"\\nLanguages: {', '.join(chw['languages_available'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 5: Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = (14, 5)\n",
        "matplotlib.rcParams['figure.facecolor'] = '#0a1628'\n",
        "matplotlib.rcParams['axes.facecolor'] = '#0a1628'\n",
        "matplotlib.rcParams['text.color'] = 'white'\n",
        "matplotlib.rcParams['axes.labelcolor'] = 'white'\n",
        "matplotlib.rcParams['xtick.color'] = 'white'\n",
        "matplotlib.rcParams['ytick.color'] = 'white'\n",
        "\n",
        "# --- 8-Week Risk Forecast ---\n",
        "weeks = [f['week'] for f in assessment['climate_forecast']]\n",
        "temps = [f['temperature'] for f in assessment['climate_forecast']]\n",
        "precips = [f['precipitation'] for f in assessment['climate_forecast']]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "ax1.plot(weeks, temps, 'o-', color='#EF4444', linewidth=2, markersize=6)\n",
        "ax1.fill_between(weeks, temps, alpha=0.15, color='#EF4444')\n",
        "ax1.set_title('Temperature Forecast (LSTM Model)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Temperature (Â°C)')\n",
        "ax1.grid(alpha=0.1)\n",
        "\n",
        "ax2.bar(weeks, precips, color='#3B82F6', alpha=0.8)\n",
        "ax2.set_title('Precipitation Forecast (Prophet Model)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Precipitation (mm)')\n",
        "ax2.grid(alpha=0.1)\n",
        "\n",
        "plt.suptitle('ClimaHealth AI â€” Dhaka, Bangladesh Climate Forecast', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- SHAP Feature Importance ---\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "categories = list(assessment['shap_summary'].keys())\n",
        "values = [v * 100 for v in assessment['shap_summary'].values()]\n",
        "colors = ['#EF4444', '#06B6D4', '#F59E0B', '#8B5CF6', '#3B82F6']\n",
        "\n",
        "bars = ax.barh(categories[::-1], values[::-1], color=colors[::-1], height=0.6)\n",
        "ax.set_xlabel('Contribution (%)')\n",
        "ax.set_title('SHAP Feature Importance â€” What Drives the Prediction?', fontsize=13, fontweight='bold')\n",
        "ax.grid(axis='x', alpha=0.1)\n",
        "\n",
        "for bar, val in zip(bars, values[::-1]):\n",
        "    ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "            f'{val:.1f}%', va='center', fontsize=11, color='white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Ensemble Model Breakdown ---\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "models = ['Climate\\nForecaster', 'Disease\\nEnsemble', 'NLP\\nDetector']\n",
        "scores = [\n",
        "    assessment['component_scores']['climate_risk'],\n",
        "    assessment['component_scores']['disease_ensemble_risk'],\n",
        "    assessment['component_scores']['nlp_signal_risk'],\n",
        "]\n",
        "weights = [40, 45, 15]\n",
        "colors = ['#EF4444', '#3B82F6', '#F59E0B']\n",
        "\n",
        "bars = ax.bar(models, scores, color=colors, width=0.5, alpha=0.85)\n",
        "\n",
        "for bar, score, weight in zip(bars, scores, weights):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "            f'{score:.0f}/100\\n(weight: {weight}%)', ha='center', fontsize=10, color='white')\n",
        "\n",
        "ax.axhline(y=assessment['risk_score'], color='white', linestyle='--', alpha=0.5, label=f\"Final Score: {assessment['risk_score']}\")\n",
        "ax.set_ylabel('Risk Score')\n",
        "ax.set_ylim(0, 110)\n",
        "ax.set_title('Ensemble Model Component Scores', fontsize=13, fontweight='bold')\n",
        "ax.legend(loc='upper right', facecolor='#0a1628', edgecolor='white')\n",
        "ax.grid(axis='y', alpha=0.1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- NLP Signal Confidence ---\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "\n",
        "headlines = [s['text'][:50] + '...' for s in assessment['nlp_signals']]\n",
        "confidences = [s['confidence'] for s in assessment['nlp_signals']]\n",
        "bar_colors = ['#EF4444' if s['is_outbreak'] else '#10B981' for s in assessment['nlp_signals']]\n",
        "\n",
        "bars = ax.barh(headlines[::-1], confidences[::-1], color=bar_colors[::-1], height=0.5)\n",
        "ax.set_xlabel('Confidence Score')\n",
        "ax.set_xlim(0, 1.1)\n",
        "ax.set_title('NLP Outbreak Signal Detection â€” News Classification', fontsize=13, fontweight='bold')\n",
        "ax.axvline(x=0.5, color='white', linestyle='--', alpha=0.3, label='Threshold')\n",
        "ax.legend(facecolor='#0a1628', edgecolor='white')\n",
        "ax.grid(axis='x', alpha=0.1)\n",
        "\n",
        "for bar, conf in zip(bars, confidences[::-1]):\n",
        "    label = 'ðŸ”´' if conf > 0.5 else 'ðŸŸ¢'\n",
        "    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2,\n",
        "            f'{label} {conf:.0%}', va='center', fontsize=10, color='white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 6: Test the NLP Detector on Custom Headlines\n",
        "Try entering your own headlines to see how the model classifies them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with your own headlines!\n",
        "test_headlines = [\n",
        "    'Massive dengue outbreak reported in Southeast Asia following record floods',\n",
        "    'Kenya highland malaria cases surge 40% as temperatures rise',\n",
        "    'Cholera spreads through flood-damaged water systems in Bangladesh',\n",
        "    'Local farmers market opens for the spring season',\n",
        "    'New school construction completed in rural district',\n",
        "    'Emergency declared as mosquito-borne disease cases triple in Lagos',\n",
        "]\n",
        "\n",
        "results = nlp_model.predict(test_headlines)\n",
        "\n",
        "print('NLP Outbreak Signal Detection Results')\n",
        "print('=' * 70)\n",
        "for r in results:\n",
        "    emoji = 'ðŸ”´ OUTBREAK' if r['is_outbreak'] else 'ðŸŸ¢ Normal  '\n",
        "    print(f\"{emoji} | conf={r['confidence']:.2f} | sev={r['severity']:8s} | {r['disease']:8s}\")\n",
        "    print(f\"   \\\"{r['text'][:65]}\\\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Step 7: Full Model Performance Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importances\n",
        "print('\\nðŸ“Š Disease Predictor â€” Top 15 Feature Importances')\n",
        "print('=' * 50)\n",
        "raw_importance = disease_model.get_feature_importance()\n",
        "for feat, imp in list(raw_importance.items())[:15]:\n",
        "    bar = 'â–ˆ' * int(imp * 200)\n",
        "    print(f'  {feat:30s} {imp*100:5.2f}% {bar}')\n",
        "\n",
        "print('\\nðŸ“° NLP Detector â€” Top Outbreak Indicator Words')\n",
        "print('=' * 50)\n",
        "top = nlp_model.get_top_features(n=10)\n",
        "for word, coef in top['outbreak_indicators'][:10]:\n",
        "    print(f'  {word:30s} coef={coef:+.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Done!\n",
        "\n",
        "You've just run the full ClimaHealth AI pipeline:\n",
        "- **3 ML models** trained on real/synthetic data\n",
        "- **Ensemble risk scoring** combining climate + disease + NLP signals\n",
        "- **SHAP explainability** showing what drives each prediction\n",
        "- **Community health worker alerts** in plain language\n",
        "\n",
        "For the interactive React dashboard, open `frontend/climahealth.jsx` in Claude.\n",
        "\n",
        "---\n",
        "*ClimaHealth AI â€” InnovAIte Hackathon 2026*"
      ]
    }
  ]
}
